{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873759dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import (\n",
    "    TextNode,\n",
    "    NodeRelationship,\n",
    "    RelatedNodeInfo,\n",
    "    ObjectType,\n",
    ")\n",
    "from typing import List, Dict\n",
    "import uuid\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from law_search.vector_db import QdrantCollection\n",
    "from llama_index.core import Settings\n",
    "import os\n",
    "\n",
    "# Hardcoded configurations\n",
    "# MODEL_NAME = \"truro7/vn-law-embedding\"\n",
    "MODEL_NAME = \"phi010402/finetune-alqac-question-generation-2\"\n",
    "MODEL_CACHE_DIR = \"./models\"\n",
    "COLLECTION_NAME = \"law_sections\"\n",
    "\n",
    "\n",
    "def process_json_content(json_content: List[Dict], file_name: str) -> List[TextNode]:\n",
    "    \"\"\"\n",
    "    Process JSON content and create TextNodes with relationships.\n",
    "\n",
    "    Args:\n",
    "        json_content: List containing the document json\n",
    "        file_name: Name of the source file\n",
    "\n",
    "    Returns:\n",
    "        List of TextNodes with established relationships\n",
    "    \"\"\"\n",
    "\n",
    "    nodes = []\n",
    "    for content in json_content:\n",
    "        section_id = list(content.keys())[0]\n",
    "        section_data = list(content.values())[0]\n",
    "        # Create text node\n",
    "        node = TextNode(\n",
    "            text=section_data,\n",
    "            id_=str(uuid.uuid4()),\n",
    "            metadata={\n",
    "                \"doc_id\": file_name,\n",
    "                \"section_id\": section_id,\n",
    "                \"title\": section_data.split(\"\\n\\n\")[0],\n",
    "            },\n",
    "        )\n",
    "        nodes.append(node)\n",
    "\n",
    "    for i, node in enumerate(nodes):\n",
    "        if i > 0:\n",
    "            node.relationships[NodeRelationship.PREVIOUS] = RelatedNodeInfo(\n",
    "                node_id=nodes[i - 1].node_id,\n",
    "                node_type=ObjectType.TEXT,\n",
    "                hash=nodes[i - 1].hash,\n",
    "            )\n",
    "        if i < len(nodes) - 1:\n",
    "            node.relationships[NodeRelationship.NEXT] = RelatedNodeInfo(\n",
    "                node_id=nodes[i + 1].node_id,\n",
    "                node_type=ObjectType.TEXT,\n",
    "                hash=nodes[i + 1].hash,\n",
    "            )\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def setup_embedding_model() -> None:\n",
    "    \"\"\"Initialize and setup the embedding model.\"\"\"\n",
    "\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        cache_folder=os.path.join(MODEL_CACHE_DIR, MODEL_NAME),\n",
    "    )\n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.chunk_size = 512\n",
    "    Settings.db = QdrantCollection(collection_name=\"law_sections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedidng\n",
    "setup_embedding_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6d6b4",
   "metadata": {},
   "source": [
    "Only run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from law_search import QdrantCollection\n",
    "\n",
    "nodes = []\n",
    "# Indexing to qdrant local db\n",
    "\n",
    "output_path = Path(\"./output\")\n",
    "for filename in output_path.glob(\"*\"):\n",
    "    print(filename.stem)\n",
    "    with open(filename, \"r\") as file:\n",
    "        json_content = json.load(file)\n",
    "    nodes.extend(process_json_content(json_content, filename.stem))\n",
    "\n",
    "Settings.db.insert_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb235db",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708b2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 2\n",
    "SPARSE_TOP_K = 12\n",
    "retriever_engine = Settings.db._index.as_retriever(\n",
    "    similarity_top_k=TOP_K,\n",
    "    sparse_top_k=SPARSE_TOP_K,\n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    node_postprocessor=[],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b324ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import QueryBundle\n",
    "\n",
    "\n",
    "def retrieve(query):\n",
    "    result_nodes = retriever_engine._retrieve(\n",
    "        QueryBundle(\n",
    "            query_str=query,\n",
    "        )\n",
    "    )\n",
    "    result_dict = {\"result\": []}\n",
    "    for node in result_nodes:\n",
    "        if node.score < 0.5:\n",
    "            if TOP_K > 1:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            result_dict[\"result\"].append(\n",
    "                {\n",
    "                    \"document\": node.node.metadata[\"doc_id\"],\n",
    "                    \"id\": node.node.metadata[\"section_id\"],\n",
    "                    \"score\": node.score,\n",
    "                    \"text\": node.node.text,\n",
    "                }\n",
    "            )\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "Một người có thể được người có quyền theo quy định của pháp luật yêu cầu Tòa án ra quyết định tuyên bố là đã chết khi người đó biệt tích trong chiến tranh sau 05 năm, kể từ ngày chiến tranh kết thúc mà vẫn không có tin tức xác thực là còn sống, đúng hay sai?\n",
    "\"\"\"\n",
    "retrieve(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6024fa",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "import json\n",
    "\n",
    "# path = \"ALQAC_2025_data/alqac25_train.json\"\n",
    "path = \"ALQAC_2025_data/alqac25_private_test_Task_1.json\"\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e92fd32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_articles = {}\n",
    "for item in json_data:\n",
    "    count = len(item[\"relevant_articles\"])\n",
    "    if count not in number_of_articles:\n",
    "        number_of_articles[count] = 1\n",
    "    else:\n",
    "        number_of_articles[count] += 1\n",
    "number_of_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f7a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(retrieved_articles, relevant_articles):\n",
    "    \"\"\"\n",
    "    Calculates precision for a single question.\n",
    "    A retrieved article is correct if its (law_id, article_id) tuple matches a relevant article.\n",
    "    \"\"\"\n",
    "    retrieved_set = {(item[\"document\"], item[\"id\"]) for item in retrieved_articles}\n",
    "    relevant_set = {(item[\"law_id\"], item[\"article_id\"]) for item in relevant_articles}\n",
    "\n",
    "    correctly_retrieved = len(retrieved_set.intersection(relevant_set))\n",
    "    total_retrieved = len(retrieved_set)\n",
    "\n",
    "    if total_retrieved == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return correctly_retrieved / total_retrieved\n",
    "\n",
    "\n",
    "def calculate_recall(retrieved_articles, relevant_articles):\n",
    "    \"\"\"\n",
    "    Calculates recall for a single question.\n",
    "    A retrieved article is correct if its (law_id, article_id) tuple matches a relevant article.\n",
    "    \"\"\"\n",
    "    retrieved_set = {(item[\"document\"], item[\"id\"]) for item in retrieved_articles}\n",
    "    relevant_set = {(item[\"law_id\"], item[\"article_id\"]) for item in relevant_articles}\n",
    "\n",
    "    correctly_retrieved = len(retrieved_set.intersection(relevant_set))\n",
    "    total_relevant = len(relevant_set)\n",
    "\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return correctly_retrieved / total_relevant\n",
    "\n",
    "\n",
    "def calculate_f2_score(precision, recall):\n",
    "    \"\"\"\n",
    "    Calculates the F2 score based on the provided formula.\n",
    "    \"\"\"\n",
    "    if (4 * precision + recall) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return (5 * precision * recall) / (4 * precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "all_precision = []\n",
    "all_recall = []\n",
    "all_f2_scores = []\n",
    "details = []\n",
    "evaluation_path = \"evaluation2\"\n",
    "os.makedirs(evaluation_path, exist_ok=True)\n",
    "\n",
    "for idx, item in enumerate(json_data):\n",
    "    query = item[\"text\"]\n",
    "    relevant_articles = item[\"relevant_articles\"]\n",
    "\n",
    "    retrieved_results = retrieve(query)\n",
    "    retrieved_articles = retrieved_results.get(\"result\", [])\n",
    "    precision = calculate_precision(retrieved_articles, relevant_articles)\n",
    "    recall = calculate_recall(retrieved_articles, relevant_articles)\n",
    "    f2 = calculate_f2_score(precision, recall)\n",
    "\n",
    "    all_precision.append(precision)\n",
    "    all_recall.append(recall)\n",
    "    all_f2_scores.append(f2)\n",
    "    details.append(\n",
    "        {\n",
    "            \"question_id\": item.get(\"question_id\", idx),\n",
    "            \"query\": query,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f2_score\": f2,\n",
    "            \"retrieved_articles\": retrieved_articles,\n",
    "            \"relevant_articles\": relevant_articles,\n",
    "        }\n",
    "    )\n",
    "\n",
    "if all_f2_scores:\n",
    "    average_f2 = sum(all_f2_scores) / len(all_f2_scores)\n",
    "    average_precision = sum(all_precision) / len(all_precision)\n",
    "    average_recall = sum(all_recall) / len(all_recall)\n",
    "    print(f\"Average Precision: {average_precision:.4f}\")\n",
    "    print(f\"Average Recall: {average_recall:.4f}\")\n",
    "    print(f\"Average F2-Score: {average_f2:.4f}\")\n",
    "else:\n",
    "    print(\"Could not calculate F2-Score, no data processed.\")\n",
    "\n",
    "with open(\n",
    "    f\"{evaluation_path}/detailed_metrics_{TOP_K}_{SPARSE_TOP_K}.json\",\n",
    "    \"w\",\n",
    "    encoding=\"utf-8\",\n",
    ") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"average_precision\": average_precision if all_f2_scores else None,\n",
    "            \"average_recall\": average_recall if all_f2_scores else None,\n",
    "            \"average_f2_score\": average_f2 if all_f2_scores else None,\n",
    "            \"details\": details,\n",
    "        },\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "- generate 5 queries -> retriev 5 times -> combine the retrieved_results\n",
    "- use some examples datasets for style transfer\n",
    "- apply reranker for post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b960d70a",
   "metadata": {},
   "source": [
    "# Test Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a3366a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
