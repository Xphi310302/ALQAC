{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4433489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"ALQAC_2025_data/alqac25_train.json\"\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "json_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064482a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_path = \"ALQAC_2025_data/alqac25_law.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = json.load(f)\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565b30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# model_name = \"DeepSeek-R1-Distill-Llama-8B\"\n",
    "# model_name = \"Qwen2.5-7B-Instruct\"\n",
    "model_name = \"Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"llama-3.1-8b-instant\"\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"llm-fpt/.env\")\n",
    ")\n",
    "\n",
    "\n",
    "class FPTLlm:\n",
    "    def __init__(self, model_name: str = \"Qwen2.5-7B-Instruct\"):\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"FPT_API_KEY\"), base_url=\"https://mkp-api.fptcloud.com\"\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def complete(self, input: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": input}],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.01,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "class GroqLlm:\n",
    "    def __init__(self, model_name: str = \"gemma2-9b-it\"):\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\"), base_url=\"https://api.groq.com/openai/v1\"\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def list_models(self):\n",
    "        return self.client.models.list().data\n",
    "\n",
    "    def complete(self, input: str) -> str:\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": input}],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.01,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "llm = FPTLlm(model_name=model_name)\n",
    "# llm = GroqLlm(model_name=model_name)\n",
    "# pprint(llm.list_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "prompt_path = \"prompts_TOT.json\"\n",
    "# prompt_path = \"prompts_vn.json\"\n",
    "with open(prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    prompts_template = json.load(file)\n",
    "prompts_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_ESSAY = \"\"\"\n",
    "Câu trả lời nên được đặt bên trong ```answer```. \n",
    "Ví dụ tham khảo:\n",
    "Ngữ cảnh: doanh nghiệp phải đền bù cho người lao động 2 tháng lương nếu doanh nghiệp đơn phương chấm dứt hợp đồng lao động.\n",
    "Câu hỏi: Doanh nghiệp phải đền bù bao nhiêu tháng lương cho người lao động nếu chấm dứt hợp đồng lao động trước thời hạn?\n",
    "Theo ngữ cảnh được cung cấp nếu như phải chấm dứt hợp đồng lao động trước thời hạn thì doanh nghiệp phải đền bù cho người lao động 2 tháng lương.\n",
    "```answer\n",
    "2 tháng.\n",
    "``` \n",
    "\"\"\"\n",
    "OUTPUT_OPTIONS = \"\"\"\n",
    "Câu trả lời (chỉ trả lời `Đúng` hoặc `Sai`) nên được đặt bên trong ```answer```.\n",
    "Ví dụ tham khảo:\n",
    "Ngữ cảnh: Người chưa đủ 18 tuổi không được uống rượu, bia.\n",
    "Câu hỏi: Người 16 tuổi có được phép uống rượu không?\n",
    "Theo ngữ cảnh được cung cấp, người 16 tuổi không được phép uống rượu.\n",
    "```answer\n",
    "Đúng.\n",
    "```\n",
    "\"\"\"\n",
    "OUTPUT_CHOICES = \"\"\"\n",
    "Câu trả lời (chỉ trả lời `A`, `B`, `C` hoặc `D`) nên được đặt bên trong ```answer```.\n",
    "Ví dụ tham khảo:\n",
    "Ngữ cảnh: Người chưa đủ 18 tuổi không được uống rượu, bia.\n",
    "Câu hỏi: Người 16 tuổi có được phép uống rượu không?\n",
    "Theo ngữ cảnh được cung cấp, người 16 tuổi không được phép uống rượu.\n",
    "```answer\n",
    "C\n",
    "``` \n",
    "\"\"\"\n",
    "\n",
    "POST_PROCESS_PROMPT = \"\"\"\n",
    "Bạn là một hệ thống trích xuất thông tin. Nhiệm vụ của bạn là **chỉ trích xuất chính xác phần thông tin được yêu cầu từ văn bản ngữ cảnh dưới đây**.\n",
    "\n",
    "### Yêu cầu:\n",
    "- **Chỉ trích xuất thông tin trả lời cho câu hỏi hoặc đoạn đầu vào.**\n",
    "- **Không thêm bất kỳ thông tin, câu văn, hay giải thích nào khác.**\n",
    "- **Không lặp lại đầu vào.**\n",
    "- **Trích xuất đúng phần nội dung trong văn bản, giữ nguyên cách diễn đạt.**\n",
    "- Nếu không tìm thấy thông tin phù hợp, trả về **\"Không tìm thấy thông tin.\"**\n",
    "\n",
    "---\n",
    "\n",
    "### Câu hỏi:\n",
    "\n",
    "Hành vi nào được coi là cản trở việc xác minh chứng cứ?\n",
    "\n",
    "---\n",
    "\n",
    "### Ví dụ:\n",
    "\n",
    "**Đầu vào:**  \n",
    "Hành vi cản trở người phiên dịch thực hiện nhiệm vụ hoặc buộc người phiên dịch dịch không trung thực, không khách quan, không đúng nghĩa (điểm 6) và Hành vi cố ý dịch sai sự thật (điểm 8) và Hành vi không cử người tham gia Hội đồng định giá theo yêu cầu của Tòa án mà không có lý do chính đáng; không tham gia thực hiện nhiệm vụ của Hội đồng định giá mà không có lý do chính đáng (điểm 9) và Hành vi cản trở người tiến hành tố tụng xem xét, thẩm định tại chỗ, quyết định định giá, quyết định trưng cầu giám định hoặc xác minh, thu thập chứng cứ khác theo quy định của Luật này (điểm 7) được coi là hành vi cản trở thu thập xác minh chứng cứ của tòa án.\n",
    "\n",
    "**Đầu ra:**  \n",
    "Dịch sai sự thật\n",
    "\n",
    "---\n",
    "\n",
    "**Đầu vào:**  \n",
    "Những vụ án hành chính không tiến hành đối thoại được là những vụ án thuộc các trường hợp sau: người khởi kiện, người bị kiện, người có quyền lợi, nghĩa vụ liên quan đã được Tòa án triệu tập hợp lệ lần thứ hai mà vẫn cố tình vắng mặt; đương sự không thể tham gia đối thoại được vì có lý do chính đáng; các bên đương sự thống nhất đề nghị không tiến hành đối thoại.\n",
    "\n",
    "**Đầu ra:**  \n",
    "Các bên đương sự đã được triệu tập lần thứ hai mà vẫn cố tình vắng mặt, hoặc không thể tham gia đối thoại, hoặc thống nhất không tiến hành đối thoại\n",
    "\n",
    "---\n",
    "\n",
    "**Đầu vào:**  \n",
    "Theo quy định của pháp luật, nguyên tắc đối thoại trong vụ án hành chính được quy định như sau: bảo đảm công khai, dân chủ, tôn trọng ý kiến của đương sự; không được ép buộc các đương sự thực hiện việc giải quyết vụ án hành chính trái với ý chí của họ; nội dung đối thoại, kết quả đối thoại thành giữa các đương sự không trái pháp luật, trái đạo đức xã hội.\n",
    "\n",
    "**Đầu ra:**  \n",
    "Nguyên tắc đối thoại là bắt buộc\n",
    "\n",
    "---\n",
    "\n",
    "**Đầu vào:**  \n",
    "Khi đối thoại thành thì Thẩm phán sẽ ra quyết định công nhận kết quả đối thoại và đình chỉ việc giải quyết vụ án, quyết định này có hiệu lực ngay lập tức và không bị kháng cáo hay kháng nghị theo thủ tục phúc thẩm.\n",
    "\n",
    "**Đầu ra:**  \n",
    "Quyết định có hiệu lực thi hành ngay và không bị kháng cáo, kháng nghị theo thủ tục phúc thẩm\n",
    "\n",
    "---\n",
    "\n",
    "**Đầu vào:**  \n",
    "Trong vụ án hành chính thì không có hòa giải mà tổ chức đối thoại để các đương sự thống nhất với nhau về việc giải quyết vụ án. Trường hợp đối thoại thành thì Thẩm phán ra quyết định công nhận kết quả đối thoại thành, đình chỉ việc giải quyết vụ án.\n",
    "\n",
    "**Đầu ra:**  \n",
    "Không có hòa giải mà tổ chức đối thoại\n",
    "\n",
    "##Câu hỏi: {question}\n",
    "**Đầu vào:** \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d76ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = []\n",
    "added_types = set()\n",
    "for sample in json_data:\n",
    "    qt = sample[\"question_type\"].lower()\n",
    "    if qt in {\"đúng/sai\", \"tự luận\", \"trắc nghiệm\"} and qt not in added_types:\n",
    "        example.append(sample)\n",
    "        added_types.add(qt)\n",
    "    if len(added_types) == 9:\n",
    "        break\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1077463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompts(\n",
    "    example,\n",
    "    corpus=corpus,\n",
    "    prompts_template=prompts_template,\n",
    "    prompt_index=0,\n",
    "    OUTPUT_OPTIONS=OUTPUT_OPTIONS,\n",
    "    OUTPUT_ESSAY=OUTPUT_ESSAY,\n",
    "    OUTPUT_CHOICES=OUTPUT_CHOICES,\n",
    "):\n",
    "    prompts = []\n",
    "    for sample in example:\n",
    "        qt = sample[\"question_type\"].lower()\n",
    "        premise = \"\"\n",
    "        try:\n",
    "            for article in sample[\"relevant_articles\"]:\n",
    "                law_id = article[\"law_id\"]\n",
    "                article_id = article[\"article_id\"]\n",
    "\n",
    "                law_corpus = next((law for law in corpus if law[\"id\"] == law_id), None)\n",
    "                if law_corpus is None:\n",
    "                    raise KeyError(f\"law_id '{law_id}' not found in corpus\")\n",
    "\n",
    "                article_corpus = next(\n",
    "                    (a for a in law_corpus[\"articles\"] if a[\"id\"] == str(article_id)),\n",
    "                    None,\n",
    "                )\n",
    "                if article_corpus is None:\n",
    "                    raise KeyError(\n",
    "                        f\"article_id '{article_id}' not found in law_id '{law_id}'\"\n",
    "                    )\n",
    "                premise += f\"\\n{article_corpus['text']}\"\n",
    "\n",
    "            if qt == \"đúng/sai\":\n",
    "                prompt = prompts_template[\"truefalse\"][prompt_index].format(\n",
    "                    premise=premise, hypothesis=sample[\"text\"]\n",
    "                )\n",
    "                prompt = OUTPUT_OPTIONS + prompt\n",
    "\n",
    "            elif qt == \"tự luận\":\n",
    "                prompt = prompts_template[\"essay\"][prompt_index].format(\n",
    "                    premise=premise, hypothesis=sample[\"text\"]\n",
    "                )\n",
    "                prompt = OUTPUT_ESSAY + prompt\n",
    "\n",
    "            elif qt == \"trắc nghiệm\":\n",
    "                prompt = prompts_template[\"options\"][prompt_index].format(\n",
    "                    premise=premise,\n",
    "                    hypothesis=sample[\"text\"],\n",
    "                    choices={\n",
    "                        \"A\": sample[\"choices\"][\"A\"],\n",
    "                        \"B\": sample[\"choices\"][\"B\"],\n",
    "                        \"C\": sample[\"choices\"][\"C\"],\n",
    "                        \"D\": sample[\"choices\"][\"D\"],\n",
    "                    },\n",
    "                )\n",
    "                prompt = OUTPUT_CHOICES + prompt\n",
    "\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"question_id\": sample[\"question_id\"],\n",
    "                    \"prompt\": prompt,\n",
    "                    \"question_type\": qt,\n",
    "                    \"question\": sample[\"text\"],\n",
    "                }\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            print(f\"Error processing sample {sample.get('question_id', '')}: {e}\")\n",
    "            print(prompt)\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def extract_output(output, question_type, question, llm=None, max_retries=2):\n",
    "    import re\n",
    "\n",
    "    def extract_truefalse(text):\n",
    "        lowered = text.lower().replace(\".\", \"\").strip()\n",
    "        if \"đúng\" in lowered:\n",
    "            return \"Đúng\"\n",
    "        if \"sai\" in lowered:\n",
    "            return \"Sai\"\n",
    "        return lowered.strip().replace(\".\", \"\")\n",
    "\n",
    "    def extract_choices(text):\n",
    "        cleaned = text.strip().replace(\".\", \"\")\n",
    "        for choice in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "            if re.match(rf\"^\\s*{choice}\\b\", cleaned, re.IGNORECASE):\n",
    "                return choice\n",
    "        match = re.search(r\"\\b([A-D])\\b\", cleaned, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "        return cleaned.strip().replace(\".\", \"\")\n",
    "\n",
    "    def extract_answer(text):\n",
    "        text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "        # Match the specific answer format for both patterns\n",
    "        match = re.search(\n",
    "            r\"(?:\\*\\*answer\\*\\*|```answer)\\s*([\\s\\S]+?)\\s*(?:```)?$\", text, re.MULTILINE\n",
    "        )\n",
    "        answer = match.group(1).strip() if match else text.strip()\n",
    "        return answer.replace(\".\", \"\")\n",
    "\n",
    "    output = extract_answer(output)\n",
    "\n",
    "    if question_type.lower() == \"đúng/sai\":\n",
    "        return extract_truefalse(output)\n",
    "\n",
    "    if question_type.lower() == \"trắc nghiệm\":\n",
    "        return extract_choices(output)\n",
    "\n",
    "    if question_type.lower() == \"tự luận\" and llm is not None:\n",
    "        for _ in range(max_retries):\n",
    "            # try to get a clean answer by prompting the LLM again for just the answer\n",
    "            try:\n",
    "                followup_prompt = POST_PROCESS_PROMPT.format(question=question) + output\n",
    "                llm_result = llm.complete(followup_prompt)\n",
    "                clean = extract_answer(llm_result)\n",
    "                if clean:\n",
    "                    return clean\n",
    "            except Exception:\n",
    "                continue\n",
    "        return output.strip()\n",
    "\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "prompts = build_prompts(\n",
    "    example,\n",
    "    corpus,\n",
    "    prompts_template,\n",
    "    prompt_index=0,\n",
    "    OUTPUT_OPTIONS=OUTPUT_OPTIONS,\n",
    "    OUTPUT_ESSAY=OUTPUT_ESSAY,\n",
    "    OUTPUT_CHOICES=OUTPUT_CHOICES,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c22717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = prompts[0]\n",
    "response = llm.complete(input[\"prompt\"])\n",
    "print(extract_output(response, input[\"question_type\"], input[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3140cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_eval_from_list(prediction_list, gold_list):\n",
    "    \"\"\"\n",
    "    Evaluate accuracy from prediction and gold answer lists.\n",
    "    Each element in both lists is a dict with keys: 'question_id' and 'answer'.\n",
    "    Returns accuracy float.\n",
    "    \"\"\"\n",
    "    gold_dict = {item[\"question_id\"]: item[\"answer\"] for item in gold_list}\n",
    "    pred_dict = {item[\"question_id\"]: item[\"answer\"] for item in prediction_list}\n",
    "    count_true = 0\n",
    "    total = len(gold_dict)\n",
    "    for qid, gold_ans in gold_dict.items():\n",
    "        pred_ans = pred_dict.get(qid)\n",
    "        if pred_ans is not None and pred_ans.lower() == gold_ans.lower():\n",
    "            count_true += 1\n",
    "    acc = count_true / total if total else 0\n",
    "    print(\"Acc = {}\".format(acc))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation with logging and JSON output for evaluation metrics\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "\n",
    "\n",
    "eval_path = \"evaluation_task2\"\n",
    "output_file = f\"result_task_2_{model_name}\"\n",
    "prompt_used_index = 0\n",
    "max_attempts = 3\n",
    "log_path = os.path.join(eval_path, \"evaluation.log\")\n",
    "os.makedirs(eval_path, exist_ok=True)\n",
    "# Attempt to delete the log file if it exists and is not locked\n",
    "try:\n",
    "    if os.path.exists(log_path):\n",
    "        os.remove(log_path)\n",
    "except PermissionError:\n",
    "    pass\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "prompts = build_prompts(\n",
    "    json_data,\n",
    "    corpus,\n",
    "    prompts_template,\n",
    "    prompt_index=prompt_used_index,\n",
    "    OUTPUT_OPTIONS=OUTPUT_OPTIONS,\n",
    "    OUTPUT_ESSAY=OUTPUT_ESSAY,\n",
    "    OUTPUT_CHOICES=OUTPUT_CHOICES,\n",
    ")\n",
    "results = []\n",
    "predictions = []\n",
    "\n",
    "for input in tqdm(prompts):\n",
    "    attempt = 0\n",
    "    success = False\n",
    "    log_entry = {\n",
    "        \"question_id\": input[\"question_id\"],\n",
    "        \"prompt\": input[\"prompt\"],\n",
    "        \"attempts\": [],\n",
    "        \"final_answer\": None,\n",
    "        \"error\": None,\n",
    "    }\n",
    "    logging.info(f\"Evaluating question_id={input['question_id']}\")\n",
    "    while attempt < max_attempts and not success:\n",
    "        try:\n",
    "            answer = llm.complete(input[\"prompt\"])\n",
    "            parsed_answer = extract_output(\n",
    "                answer, input[\"question_type\"], input[\"question\"]\n",
    "            )\n",
    "            log_entry[\"attempts\"].append(\n",
    "                {\"attempt\": attempt + 1, \"output\": parsed_answer, \"error\": None}\n",
    "            )\n",
    "            log_entry[\"final_answer\"] = parsed_answer\n",
    "            success = True\n",
    "            logging.info(\n",
    "                f\"Success on attempt {attempt + 1} for question_id={input['question_id']}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log_entry[\"attempts\"].append(\n",
    "                {\"attempt\": attempt + 1, \"output\": None, \"error\": str(e)}\n",
    "            )\n",
    "            log_entry[\"error\"] = str(e)\n",
    "            logging.error(\n",
    "                f\"Error on attempt {attempt + 1} for question_id={input['question_id']}: {e}\"\n",
    "            )\n",
    "            attempt += 1\n",
    "    results.append(log_entry)\n",
    "    predictions.append(\n",
    "        {\"question_id\": input[\"question_id\"], \"answer\": log_entry[\"final_answer\"]}\n",
    "    )\n",
    "# Save predictions and evaluation metrics in a single JSON file\n",
    "acc = acc_eval_from_list(predictions, json_data)\n",
    "eval_metrics = {\n",
    "    \"accuracy\": acc,\n",
    "    \"total_questions\": len(json_data),\n",
    "    \"evaluated_questions\": len(predictions),\n",
    "}\n",
    "pairs = [\n",
    "    {\n",
    "        \"question_id\": pred[\"question_id\"],\n",
    "        \"predict\": pred[\"answer\"],\n",
    "        \"gold\": next(\n",
    "            (\n",
    "                item[\"answer\"]\n",
    "                for item in json_data\n",
    "                if item[\"question_id\"] == pred[\"question_id\"]\n",
    "            ),\n",
    "            None,\n",
    "        ),\n",
    "    }\n",
    "    for pred in predictions\n",
    "]\n",
    "output = {\"evaluation\": eval_metrics, \"results\": pairs}\n",
    "with open(\n",
    "    os.path.join(eval_path, \"result_task_2_baseline_ToT.json\"), \"w\", encoding=\"utf-8\"\n",
    ") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logging.info(\"Evaluation completed and results written. Accuracy: {:.4f}\".format(acc))\n",
    "\n",
    "\n",
    "acc = acc_eval_from_list(predictions, json_data)\n",
    "eval_metrics = {\n",
    "    \"accuracy\": acc,\n",
    "    \"total_questions\": len(json_data),\n",
    "    \"evaluated_questions\": len(predictions),\n",
    "}\n",
    "pairs = [\n",
    "    {\n",
    "        \"question_id\": pred[\"question_id\"],\n",
    "        \"predict\": pred[\"answer\"],\n",
    "        \"gold\": next(\n",
    "            (\n",
    "                item[\"answer\"]\n",
    "                for item in json_data\n",
    "                if item[\"question_id\"] == pred[\"question_id\"]\n",
    "            ),\n",
    "            None,\n",
    "        ),\n",
    "    }\n",
    "    for pred in predictions\n",
    "]\n",
    "output = {\"evaluation\": eval_metrics, \"results\": pairs}\n",
    "with open(os.path.join(eval_path, output_file), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "logging.info(\"Evaluation completed and results written. Accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521901f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
